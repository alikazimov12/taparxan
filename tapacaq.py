import asyncio
import json
import random
import requests
import datetime
from bs4 import BeautifulSoup
from aiogram import Bot, Dispatcher, types
from faker import Faker
from datetime import datetime as dt, timezone, timedelta

# ================= CONFIG =================

TOKEN = "8427693315:AAHrqQKu1ABD_dZcJA8PVF6_l66owypoW6c"
BASE_URL = "https://tap.az/elanlar/elektronika/noutbuklar"

SUBSCRIBERS_FILE = "subscribers.json"
SEEN_FILE = "seen_ads.json"

CHECK_INTERVAL_MIN = 50
CHECK_INTERVAL_MAX = 70

fake = Faker()
session = requests.Session()

HEADERS_BASE = {
    "Accept": "text/html,application/xhtml+xml",
    "Accept-Language": "az,en;q=0.9"
}

bot = Bot(token=TOKEN)
dp = Dispatcher()

subscribers = set()
seen_ads = set()

# ================= LOG =================

def log(msg, level="INFO"):
    now = dt.now().strftime("%H:%M:%S")
    print(f"[{now}] [{level}] {msg}")

# ================= TIME =================

def az_now():
    return dt.now(timezone.utc).astimezone(
        timezone(timedelta(hours=4))
    ).replace(tzinfo=None)

# ================= HELPERS =================

def random_headers():
    h = HEADERS_BASE.copy()
    h["User-Agent"] = fake.user_agent()
    return h

def load_json(path):
    try:
        with open(path, "r") as f:
            return set(json.load(f))
    except:
        return set()

def save_json(path, data):
    with open(path, "w") as f:
        json.dump(list(data), f)

subscribers = load_json(SUBSCRIBERS_FILE)
seen_ads = load_json(SEEN_FILE)

# ================= HTTP (403/429 G√ñR√úN∆èN) =================

def safe_get(url, **kwargs):
    try:
        r = session.get(url, **kwargs)
        log(f"HTTP {r.status_code} ‚Üí {r.url}", "HTTP")

        if r.status_code == 403:
            log("403 FORBIDDEN ‚Äì IP / UA FLAGGED ‚ö†Ô∏è", "ERROR")
        elif r.status_code == 429:
            log("429 TOO MANY REQUESTS ‚Äì RATE LIMIT ‚ö†Ô∏è", "ERROR")

        r.raise_for_status()
        return r

    except requests.exceptions.RequestException as e:
        log(f"REQUEST ERROR ‚Üí {e}", "ERROR")
        return None

# ================= SCRAPER =================

def is_shop_ad_sync(url):
    r = safe_get(url, headers=random_headers(), timeout=10)
    if not r:
        log("Shop yoxlamasƒ± alƒ±nmadƒ±", "ERROR")
        return False

    soup = BeautifulSoup(r.text, "html.parser")
    if soup.select_one('a[data-stat="shop-ad-go-shop-btn"]'):
        log(f"Maƒüaza elanƒ± √ßƒ±xarƒ±ldƒ± ‚Üí {url}", "SHOP")
        return True
    return False

def fetch_page_sync():
    log("Tap.az s…ôhif…ôsi √ß…ôkilir...")

    r = safe_get(
        BASE_URL,
        headers=random_headers(),
        params={"order": "newest"},
        timeout=15
    )

    if not r:
        log("Listing s…ôhif…ôsi alƒ±nmadƒ±", "ERROR")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    cards = soup.select(".products-i")
    log(f"S…ôhif…ôd…ô {len(cards)} kart tapƒ±ldƒ±")

    ads = []

    for card in cards:
        link = card.select_one("a[href]")
        date_div = card.select_one(".products-created")

        if not link or not date_div:
            continue

        if "bug√ºn" not in date_div.text.lower():
            log("K√∂hn…ô elan skip edildi", "SKIP")
            continue

        try:
            hour_min = date_div.text.split(",")[-1].strip()
            ad_time = datetime.datetime.combine(
                az_now().date(),
                datetime.datetime.strptime(hour_min, "%H:%M").time()
            )
        except Exception as e:
            log(f"Vaxt parse x…ôtasƒ± ‚Üí {e}", "ERROR")
            continue

        title = card.select_one(".products-name")
        price = card.select_one(".products-price .price-val")

        ad = {
            "url": "https://tap.az" + link["href"],
            "title": title.text.strip() if title else "No title",
            "price": price.text.strip() if price else "No price",
            "time": ad_time
        }

        log(f"TAPILDI ‚Üí {ad['title']} | {ad['price']}")
        ads.append(ad)

    log(f"Uyƒüun elan sayƒ±: {len(ads)}")
    return ads

async def fetch_page():
    return await asyncio.to_thread(fetch_page_sync)

async def is_shop_ad(url):
    return await asyncio.to_thread(is_shop_ad_sync, url)

# ================= BOT =================

@dp.message()
async def start_cmd(message: types.Message):
    subscribers.add(message.chat.id)
    save_json(SUBSCRIBERS_FILE, subscribers)
    await message.answer("‚úÖ Yeni elanlara abun…ô oldun.")

async def monitor_loop():
    global seen_ads

    while True:
        log("Yeni elanlar yoxlanƒ±lƒ±r...")

        try:
            ads = await fetch_page()
        except Exception as e:
            log(f"Scrape crash ‚Üí {e}", "ERROR")
            ads = []

        for ad in ads:
            if ad["url"] in seen_ads:
                log(f"Artƒ±q g√∂r√ºl√ºb ‚Üí {ad['url']}", "SEEN")
                continue

            if await is_shop_ad(ad["url"]):
                continue

            seen_ads.add(ad["url"])
            save_json(SEEN_FILE, seen_ads)

            log(f"YENƒ∞ ELAN ‚Üí {ad['title']} | {ad['price']}", "NEW")

            text = f"üÜï Yeni elan:\n{ad['title']} | {ad['price']}\n{ad['url']}"

            for chat_id in subscribers:
                try:
                    await bot.send_message(chat_id, text)
                except Exception as e:
                    log(f"Telegram error ‚Üí {e}", "ERROR")

        log("D√∂vr bitdi\n")
        await asyncio.sleep(random.randint(CHECK_INTERVAL_MIN, CHECK_INTERVAL_MAX))

async def main():
    await asyncio.gather(
        dp.start_polling(bot),
        monitor_loop()
    )

if __name__ == "__main__":
    asyncio.run(main())
